GLOSSARY (ALPHABETIC ORDER)

---------------------------------------

Artificial Neural Network: Computational models inspired by the human brain, made up of interconnected nodes (neurons) that process data and learn patterns for tasks like classification or prediction.

Bayesian Network: A probabilistic graphical model that represents relationships among variables using nodes and directed edges, allowing reasoning under certainty.

Cache: A small fast memory that stores frequently accessed data to speed up processing and reduce access time to main memory or storage.

Commercial Applications: Software or systems developed for business use or profit, often sold or licensed to customers.

Computational Overhead: The extra processing time and resources required by a system to manage tasks, beyond the time spent performing the actual computation.

Data Level Approximation: A technique that reduces the amount or precision of data to lower computational cost or power usage while maintaining acceptable accuracy for the task.

DVFS (Dynamic Voltage and Frequency Scaling): A power management technique that adjusts a processor's voltage and frequency basede on workload to save energy and reduce heat.

eBPF (extended Berkeley Packet Filter): eBPF is a kernel technology that allows safely running custom programs inside the operating system to monitor, trace and extend kernel behavior without changing its code.

IPC (Inter-Process Communication): A mechanism that allows different processes in operating system to exchange data and coordinate their actions.

Kernel: The core part of an operating system that manages hardware, system resources and communication between hardware and software.

Moore's Law: The observation that the number of transistors on a microchip roughly doubles about every two years, leading to increased computing power and efficiency.

Overfitting: A modeling error in machine learning where a model learns the training data too closely, capturing noise instead of general patterns, which reduces its accuracy on new data.

Perceptron: The simplest type of artificial neural network that makes decisions by combining weighted inputs and applying an activation function to classify data.

Principal Components Analysis (PCA): A dimensionality reduction technique that transforms data into a smaller set of uncorrelated variables called principal components, capturing the most important variance in the data.     

Proportional-Integral: A type of control algorithm that combines proportional and integral actions to minimize error by correcting both the present and accumulated past differences between desired and actual values.

Regularization: A technique used in machine learning to reduce overfitting by adding a penalty to large or complex model parameters.

Scheduling Overhead: The extra time and resources the system spends managing task switching and scheduling, rathen than executing actual work.

Semi-Supervised Learning: A machine learning approach that uses a small amount of labeled data together with a large amount of unlabeled data to improve learning accuracy.

Supervised Learning: A type of machine learning where a model is trained on labeled data to predict outcomes or classify new outputs.

Support Vector Machines (SVMs): A supervised learning algorithm that finds the best boundary (hyperplane) to separate data into different classes with the maximum margin.

Unsupervised Learning: A type of machine learning that finds hidden patterns or groupings in unlabeled data without predefined outputs.